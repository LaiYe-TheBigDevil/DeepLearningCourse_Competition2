{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92e21c22",
   "metadata": {},
   "source": [
    "# The Data Collators for Contrastive Learning\n",
    "### This notebook is for reference only and does NOT the correspond to the final submission in the Kaggle Competition!\n",
    "### To run this notebook, move this to the master directory of the project, otherwise it may NOT run correctly!\n",
    "### The notebook is INCOMPLETE. We had no time for training the regression model for contrastive learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57af187d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\dl\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\ProgramData\\anaconda3\\envs\\dl\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import RobertaModel, RobertaTokenizer, TrainingArguments, Trainer, DataCollatorWithPadding, RobertaForSequenceClassification\n",
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "from datasets import load_dataset, Dataset, ClassLabel\n",
    "import pickle\n",
    "import nlpaug.augmenter.word as naw\n",
    "import nlpaug.flow as nafc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee98c009",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function preprocess_with_augmentation at 0x00000223221A8F70> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 120000/120000 [05:02<00:00, 396.04 examples/s]\n"
     ]
    }
   ],
   "source": [
    "base_model = 'roberta-base'\n",
    "\n",
    "dataset = load_dataset('ag_news', split='train')\n",
    "tokenizer = RobertaTokenizer.from_pretrained(base_model)\n",
    "\n",
    "pipeline = nafc.Sometimes([\n",
    "    naw.SynonymAug(aug_src='wordnet'),\n",
    "    naw.RandomWordAug(action='delete', aug_p=0.1),\n",
    "    naw.RandomWordAug(action='swap', aug_p=0.05),\n",
    "])\n",
    "\n",
    "def create_views(text):\n",
    "    aug1 = pipeline.augment(text)\n",
    "    aug2 = pipeline.augment(text)\n",
    "\n",
    "    # If pipeline returns list of one string instead of string, fix it\n",
    "    if isinstance(aug1, list): aug1 = aug1[0]\n",
    "    if isinstance(aug2, list): aug2 = aug2[0]\n",
    "    return [aug1, aug2]\n",
    "\n",
    "def preprocess_with_augmentation(examples):\n",
    "    views1 = []\n",
    "    views2 = []\n",
    "    for text in examples['text']:\n",
    "        aug1, aug2 = create_views(text)\n",
    "        views1.append(aug1)\n",
    "        views2.append(aug2)\n",
    "\n",
    "    max_seq_len = 363  # Maximum length for tokenized AG news texts is 362\n",
    "\n",
    "    tokens1 = tokenizer(views1, truncation=True, padding=\"max_length\", max_length=max_seq_len)\n",
    "    tokens2 = tokenizer(views2, truncation=True, padding=\"max_length\", max_length=max_seq_len)\n",
    "\n",
    "    return {\n",
    "        'input_ids_view1': tokens1['input_ids'],\n",
    "        'attention_mask_view1': tokens1['attention_mask'],\n",
    "        'input_ids_view2': tokens2['input_ids'],\n",
    "        'attention_mask_view2': tokens2['attention_mask'],\n",
    "        'labels': examples['label']\n",
    "    }\n",
    "\n",
    "tokenized_dataset = dataset.map(\n",
    "    preprocess_with_augmentation,\n",
    "    batched=True,\n",
    "    remove_columns=[\"text\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffe2c344",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataCollatorForContrastiveLearning:\n",
    "    def __init__(self, tokenizer, max_length=None):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.pad = DataCollatorWithPadding(tokenizer, padding=\"max_length\", max_length=max_length)\n",
    "\n",
    "    def __call__(self, features):\n",
    "        # Split inputs by view\n",
    "        features_view1 = [\n",
    "            {'input_ids': f['input_ids_view1'], 'attention_mask': f['attention_mask_view1']} for f in features\n",
    "        ]\n",
    "        features_view2 = [\n",
    "            {'input_ids': f['input_ids_view2'], 'attention_mask': f['attention_mask_view2']} for f in features\n",
    "        ]\n",
    "\n",
    "        # Pad both views\n",
    "        batch1 = self.pad(features_view1)\n",
    "        batch2 = self.pad(features_view2)\n",
    "\n",
    "        # Stack them: [batch_size, 2, seq_len]\n",
    "        input_ids = torch.stack([batch1['input_ids'], batch2['input_ids']], dim=1)\n",
    "        attention_mask = torch.stack([batch1['attention_mask'], batch2['attention_mask']], dim=1)\n",
    "\n",
    "        # Get labels\n",
    "        labels = torch.tensor([f['labels'] for f in features], dtype=torch.long)\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': labels\n",
    "        }\n",
    "    \n",
    "class DataCollatorForClassification:\n",
    "    def __init__(self, tokenizer, max_length=None):\n",
    "        self.pad = DataCollatorWithPadding(tokenizer, padding=\"max_length\", max_length=max_length)\n",
    "\n",
    "    def __call__(self, features):\n",
    "        # import pdb\n",
    "        # pdb.set_trace()\n",
    "        features_view1 = [\n",
    "            {\n",
    "                'input_ids': f['input_ids_view1'],\n",
    "                'attention_mask': f['attention_mask_view1']\n",
    "            }\n",
    "            for f in features\n",
    "        ]\n",
    "\n",
    "        batch = self.pad(features_view1)\n",
    "        batch['labels'] = torch.tensor([f['labels'] for f in features], dtype=torch.long)\n",
    "        return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4404cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DebugCollator:\n",
    "    def __init__(self, base_collator):\n",
    "        self.base_collator = base_collator\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        print(\"Collator received batch of size:\", len(batch))\n",
    "        return self.base_collator(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd626fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForContrastiveLearning(tokenizer, max_length=363)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72f6a938",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForClassification(tokenizer, max_length=363)\n",
    "# data_collator = DebugCollator(data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4f21d03b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Collator received batch of size: 4\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "batch = next(iter(DataLoader(tokenized_dataset, batch_size=4, collate_fn=data_collator)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3e8dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ”ï¸ Augmented and tokenized dataset ready. Number of labels: 4, Classes: ['World', 'Sports', 'Business', 'Sci/Tech']\n"
     ]
    }
   ],
   "source": [
    "# Extract the number of classess and their names\n",
    "class_names = dataset.features[\"label\"].names\n",
    "num_labels = len(class_names)\n",
    "id2label = {i: label for i, label in enumerate(class_names)}\n",
    "\n",
    "print(f\"âœ”ï¸ Augmented and tokenized dataset ready. Number of labels: {num_labels}, Classes: {class_names}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be539417",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = RobertaForSequenceClassification.from_pretrained(\n",
    "    base_model,\n",
    "    id2label=id2label) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "babf1b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the original training set\n",
    "split_datasets = tokenized_dataset.train_test_split(test_size=640, seed=42)\n",
    "train_dataset = split_datasets['train']\n",
    "eval_dataset = split_datasets['test']\n",
    "train_dataset = train_dataset.remove_columns(['label'])\n",
    "eval_dataset = eval_dataset.remove_columns(['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4264047a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids_view1', 'attention_mask_view1', 'input_ids_view2', 'attention_mask_view2', 'labels'],\n",
       "    num_rows: 119360\n",
       "})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2587d3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, f in enumerate(train_dataset):\n",
    "    if 'input_ids_view1' not in f:\n",
    "        print(f\"ðŸš« Sample {i} missing 'input_ids_view1'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "006d6704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PEFT Config\n",
    "peft_config = LoraConfig(\n",
    "    r=2,\n",
    "    lora_alpha=4,\n",
    "    lora_dropout=0.05,\n",
    "    bias = 'none',\n",
    "    target_modules = ['query'],\n",
    "    task_type=\"SEQ_CLS\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "829a4dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model = get_peft_model(model, peft_config)\n",
    "# peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37c509f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PEFT Model\n",
      "trainable params: 630,532 || all params: 125,279,240 || trainable%: 0.5033\n"
     ]
    }
   ],
   "source": [
    "print('PEFT Model')\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267650fe",
   "metadata": {},
   "source": [
    "### Test the data collators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "26d3cc8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Collator received batch of size: 4\n",
      "tensor(1.3891, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(DataLoader(train_dataset, batch_size=4, collate_fn=data_collator)))\n",
    "\n",
    "peft_model.train()\n",
    "output = peft_model(\n",
    "    input_ids=batch['input_ids'].to(peft_model.device),\n",
    "    attention_mask=batch['attention_mask'].to(peft_model.device),\n",
    "    labels=batch['labels'].to(peft_model.device)\n",
    ")\n",
    "print(output.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b16bfd2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.4454, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(DataLoader(eval_dataset, batch_size=4, collate_fn=data_collator)))\n",
    "\n",
    "peft_model.eval()\n",
    "output = peft_model(\n",
    "    input_ids=batch['input_ids'].to(peft_model.device),\n",
    "    attention_mask=batch['attention_mask'].to(peft_model.device),\n",
    "    labels=batch['labels'].to(peft_model.device)\n",
    ")\n",
    "print(output.loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd0d46d",
   "metadata": {},
   "source": [
    "## Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d433cef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To track evaluation accuracy during training\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': accuracy\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a6d1f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import EarlyStoppingCallback\n",
    "# Setup Training args\n",
    "output_dir = \"results7\"\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    report_to=None,\n",
    "    eval_strategy='steps',\n",
    "    eval_steps=1000,\n",
    "    save_strategy='steps',\n",
    "    save_steps=1000,\n",
    "    logging_steps=1000,\n",
    "    learning_rate=1e-4,\n",
    "    num_train_epochs=5,\n",
    "    use_cpu=False,\n",
    "    dataloader_num_workers=0,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    optim=\"adamw_torch\",\n",
    "    gradient_checkpointing=False,\n",
    "    gradient_checkpointing_kwargs={'use_reentrant':True},\n",
    "    load_best_model_at_end=True,\n",
    "    remove_unused_columns = False\n",
    ")\n",
    "\n",
    "def get_trainer(model):\n",
    "      return  Trainer(\n",
    "          model=model,\n",
    "          args=training_args,\n",
    "          compute_metrics=compute_metrics,\n",
    "          train_dataset=train_dataset,\n",
    "          eval_dataset=eval_dataset,\n",
    "          data_collator=data_collator,\n",
    "          callbacks=[EarlyStoppingCallback(early_stopping_patience=5)],\n",
    "      )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d666d208",
   "metadata": {},
   "source": [
    "### Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b95f7617",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mly2414\u001b[0m (\u001b[33mly2414-new-york-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Documents\\phd\\DL\\project2\\wandb\\run-20250416_005615-e0o9vgle</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ly2414-new-york-university/huggingface/runs/e0o9vgle' target=\"_blank\">results7</a></strong> to <a href='https://wandb.ai/ly2414-new-york-university/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ly2414-new-york-university/huggingface' target=\"_blank\">https://wandb.ai/ly2414-new-york-university/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ly2414-new-york-university/huggingface/runs/e0o9vgle' target=\"_blank\">https://wandb.ai/ly2414-new-york-university/huggingface/runs/e0o9vgle</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='26000' max='37300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [26000/37300 1:25:58 < 37:22, 5.04 it/s, Epoch 3/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.569700</td>\n",
       "      <td>0.336833</td>\n",
       "      <td>0.896875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.347300</td>\n",
       "      <td>0.332048</td>\n",
       "      <td>0.887500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.325500</td>\n",
       "      <td>0.328261</td>\n",
       "      <td>0.895312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.306200</td>\n",
       "      <td>0.320629</td>\n",
       "      <td>0.890625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.306700</td>\n",
       "      <td>0.308067</td>\n",
       "      <td>0.885938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.289300</td>\n",
       "      <td>0.317132</td>\n",
       "      <td>0.885938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.289500</td>\n",
       "      <td>0.304030</td>\n",
       "      <td>0.895312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.287800</td>\n",
       "      <td>0.287343</td>\n",
       "      <td>0.898438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.277600</td>\n",
       "      <td>0.281952</td>\n",
       "      <td>0.898438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.268600</td>\n",
       "      <td>0.283971</td>\n",
       "      <td>0.903125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.272600</td>\n",
       "      <td>0.270011</td>\n",
       "      <td>0.909375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.265400</td>\n",
       "      <td>0.264352</td>\n",
       "      <td>0.907813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.258600</td>\n",
       "      <td>0.265468</td>\n",
       "      <td>0.914062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.254900</td>\n",
       "      <td>0.266324</td>\n",
       "      <td>0.909375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.268700</td>\n",
       "      <td>0.260241</td>\n",
       "      <td>0.909375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.256000</td>\n",
       "      <td>0.261436</td>\n",
       "      <td>0.909375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.244200</td>\n",
       "      <td>0.269482</td>\n",
       "      <td>0.906250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.257100</td>\n",
       "      <td>0.263088</td>\n",
       "      <td>0.914062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.252000</td>\n",
       "      <td>0.255076</td>\n",
       "      <td>0.914062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.253100</td>\n",
       "      <td>0.258994</td>\n",
       "      <td>0.909375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.250800</td>\n",
       "      <td>0.248617</td>\n",
       "      <td>0.912500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.250200</td>\n",
       "      <td>0.251113</td>\n",
       "      <td>0.915625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>0.240800</td>\n",
       "      <td>0.251192</td>\n",
       "      <td>0.910937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.244600</td>\n",
       "      <td>0.248980</td>\n",
       "      <td>0.920312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.251200</td>\n",
       "      <td>0.252342</td>\n",
       "      <td>0.918750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.236600</td>\n",
       "      <td>0.249278</td>\n",
       "      <td>0.920312</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "peft_lora_finetuning_trainer = get_trainer(peft_model)\n",
    "\n",
    "result = peft_lora_finetuning_trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
